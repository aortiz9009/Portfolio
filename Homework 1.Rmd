---
title: "Homework 1"
author: "Arturo Ortiz"
date: "2023-10-23"
output: word_document
---

{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


---
title: "Matrix Example"
output: html_document
---
# Problem 1
# Part A
## There are n rows in an n x 3 matrix. There are Three columns in a n X 3 matrix. 
##Below is an example of a 4 x 3 matrix. 

n <- 4
A <- matrix(c(4, 6, 2, -1, 0, 3, 7, 1, 8, -5, 6, 10), nrow = n, ncol = 3, byrow = TRUE)
print(A)

# Part B
## Below is a 3 x2 matrix. 

B <- matrix(c(6, 2, 1, -5, 1, 3), nrow = 3, ncol = 2)
print(B)


# Problem 2
# Part A
## The expression is solvable. See below

matrix1 <- matrix(c(2, -1, 1, 2), nrow = 2, byrow = TRUE)
matrix2 <- matrix(c(1, 1, 2, 2, 0, 1), nrow = 2, byrow = TRUE)
result <- matrix1 %*% matrix2
print(result)


# Part B
## The expression is solvable. See below

matrixb_1 <- matrix(c(1, 0, 2, 1, -2, 1), nrow = 3, byrow = TRUE)
matrixb_2 <- matrix(c(1, 1, 2, 2, 0, 1), nrow = 2, byrow = TRUE)
result <- matrixb_1 %*% matrixb_2
print(result)

# Part C
## The expression is not solvable. The number of columns in the first matrix must match the number of rows in the second matrix. The expression does not meet that need. 

matrixc_1 <- matrix(c(1, 0, 2, 1, -2, 1), nrow = 3, ncol = 2)
matrixc_2 <- matrix(c(1, -1, 0), nrow = 1, ncol = 3)

resultc <- matrixc_1 %*% matrixc_2
print(resultc)

# Problem 3
# Part A
## This is solvalble. See below

a <- matrix(c(2, 1, 1, 2), nrow = 2, ncol = 2, byrow = TRUE)
result <- round(a %*% solve(a), 10)
print(result)

# Part B
## This is not solvable. The determinate of the matrix is zero. 

b <- matrix(c(2, 0, 1, 0), nrow = 2, ncol = 2, byrow = TRUE)
result <- round(b %*% solve(b), 10)
print(result)


# Problem 4
# Import Data sets
women <- read.csv("https://raw.githubusercontent.com/EricBrownTTU/ISQS6350/main/womens_track.csv")
men <- read.csv("https://raw.githubusercontent.com/EricBrownTTU/ISQS6350/main/mens_track.csv")

# Part A
cov(women[, 1:7])
cor(women[, 1:7])

# Part B
cov(men[,1:8])
cor(men[,1:8])

women
men

# Part C
women_five <- women[1:5, ]
men_five <- men[1:5, ]

common_columns <- c("m100", "m200", "m400", "m800", "m1500", "m3000", "marathon")

women_common <- women_five[, common_columns]
men_common <- men_five[, common_columns]

combined_data <- rbind(women_common, men_common)

distance_matrix <- dist(combined_data[, -8], method = "euclidean")

distance_matrix <- round(distance_matrix, 2)

print(distance_matrix)

# Part D
The most similar entries are in row 4 and column 3, with a distance of 2.11. 
This is because these entries have the smallest euclidian distance. 
The most dissimilar entries are in row 7 and column , with a distance of 42.81.
This is because these entries have the largest euclidean distance. 

# Problem 5
# Part A

matrix_5a <- matrix(c(
  3.8778, 2.8110, 3.1480, 3.5062,
  2.8110, 2.1210, 2.2669, 2.5690,
  3.1480, 2.2669, 2.6550, 2.8341,
  3.5062, 2.5690, 2.8341, 3.2352
), nrow = 4, ncol = 4)

cor_matrix <- cov2cor(matrix_5a)

# Part B
cov_1x2 <- matrix_5a[1, 2]
std_dev_1 <- sqrt(matrix_5a[1, 1])
std_dev_2 <- sqrt(matrix_5a[2, 2])

correlation_1x2 <- cov_1x2 / (std_dev_1 * std_dev_2)

print(correlation_1x2)

The correlation is 0.9801619


# Problem 6
# Part A
The jitter function is useful in this scenario because it may effectively demonstrate the correlation between two separate values originating from various variables, each of which contains numerous values inside the dataset.

grad <- read.csv("https://raw.githubusercontent.com/EricBrownTTU/ISQS6350/main/pgs.csv")
grad

library(ggplot2)
ggplot(data = grad, aes(x = jitter(FacTeaching), y = jitter(FacKnowledge))) +
  geom_point() +
  labs(x = "FacTeaching", y = "FacKnowledge") +
  ggtitle("Scatterplot of FacTeaching vs. FacKnowledge")
  
# Part B
## New Data Frame
new_data_frame <- grad[, c("FacTeaching", "FacKnowledge", "Housing")]

new_data_frame

# Part C
na_counts <- sapply(new_data_frame, function(x) sum(is.na(x)))

na_counts

cleaned_data <- na.omit(new_data_frame)

```There are 17 NA values in FacTeaching, 16 NA values in FacKnowledge 
and 317 NA values in Housing```

#Part C(i)

complete_data <- cleaned_data[complete.cases(cleaned_data), ]

correlation_matrix_complete <- cor(complete_data)

print(correlation_matrix_complete)

# Part C(ii)

correlation_matrix_available <- cor(new_data_frame, use = "complete.obs")

print(correlation_matrix_available)

# Part C(iii)

correlation_matrix_ml <- cor(new_data_frame, use = "pairwise.complete.obs")

print(correlation_matrix_ml)

```The correlation matrix values derived from the entire and available case analyses are consistent, however,there is some deviation when compared to the results produced from the maximum likelihood estimate approach. Based on this observation, available-case analysis is preferred because it successfully manages missing values while providing results similar to those obtained after removing the missing-value observations.```


# Problem 7
# load library
library(MVA)

# upload dataset
fish <- read.csv("https://raw.githubusercontent.com/EricBrownTTU/ISQS6350/main/fish.csv")

#  subsets of the data
data_subset <- fish[, c("weight", "length3")]
data_subset_2 <- fish[, c("weight", "hgtpct")]

# Remove rows with missing values
data_subset <- na.omit(data_subset)
data_subset_2 <- na.omit(data_subset_2)

# function to generate bivariate boxplots
bivariate_boxplot <- function(data, x_label, y_label) {
  bvbox(data, xlab = x_label, ylab = y_label)
}

# bivariate boxplots for weight vs. length3
bivariate_boxplot(data_subset, "Weight", "Length3")

# bivariate boxplots for weight vs. hgtpct
bivariate_boxplot(data_subset_2, "Weight", "Hgtpct")

# original data and impute missing values with medians
fishn <- fish
for (n in 1:ncol(fishn)) {
  fishn[is.na(fishn[, n]), n] <- median(fishn[, n], na.rm = TRUE)
}

# correlations for the original data
cor_original_weight_length3 <- cor(fishn$weight, fishn$length3)
cor_original_weight_hgtpct <- cor(fishn$weight, fishn$hgtpct)

# remove outliers
outliers <- match(c("100", "101", "102", "103", "104", "153", "155", "157"), rownames(fishn))
data_without_outliers_weight_length3 <- fishn[-outliers, c("weight", "length3")]
cor_without_outliers_weight_length3 <- cor(data_without_outliers_weight_length3)

newoutliers <- match(c("100", "101", "102", "103"), rownames(fishn))
data_without_outliers_weight_hgtpct <- fishn[-newoutliers, c("weight", "hgtpct")]
cor_without_outliers_weight_hgtpct <- cor(data_without_outliers_weight_hgtpct)






# Problem 8
swiss <- read.csv("https://raw.githubusercontent.com/EricBrownTTU/ISQS6350/main/swiss.csv")

# a. 

library(ggplot2)


# Calculation for kernel density estimates for top_margin and diag_length
density_top_margin <- density(swiss$top_margin, bw = 1)
density_diag_length <- density(swiss$diag_length, bw = 1)

# Plot kernel density estimates for top_margin
ggplot(data = data.frame(x = density_top_margin$x, y = density_top_margin$y),
       aes(x = x, y = y)) +
  geom_line() +
  labs(title = "Kernel Density Estimate of Top Margin",
       x = "Length (mm)", y = "Density")

# Plot kernel density estimates for diag_length
ggplot(data = data.frame(x = density_diag_length$x, y = density_diag_length$y),
       aes(x = x, y = y)) +
  geom_line() +
  labs(title = "Kernel Density Estimate of Diagonal Length",
       x = "Diagonal Length (mm)", y = "Density")
       

# b. 
library(MASS)
# Bivariate Gaussian Kernel Density Estimation
kde_bandwidth <- 1  # Use the same bandwidth as in part (a)

# The Estimate for bivariate density using a Gaussian kernel
kde_data <- kde2d(swiss$top_margin, swiss$diag_length, n = 100, h = c(kde_bandwidth, kde_bandwidth))

# contour plot
contour(kde_data, main = "Bivariate Density - Contour Plot", xlab = "Top Margin", ylab = "Diagonal Length")

# perspective plot
persp(kde_data, main = "Bivariate Density - Perspective Plot", xlab = "Top Margin", ylab = "Diagonal Length", zlab = "Density")


# c. 
#Scatterplot and Color Coding
ggplot(swiss, aes(x = top_margin, y = diag_length, color = note)) +
  geom_point() +
  labs(title = "Scatterplot of Top Margin vs. Diagonal Length", x = "Top Margin", y = "Diagonal Length") +
  scale_color_manual(values = c("genuine" = "blue", "fake" = "red")) +
  theme_minimal()


# Problem 9
# Load package

data(BCG, package = "HSAUR2")

# Exclude the year and the study number (columns 1 and 7)
bcg_data <- BCG[, -c(1, 7)]

# a. 
#Find the column-means vector
column_means <- colMeans(bcg_data)
cat("Column Means:\n")
print(column_means)

# b. 
#Find the covariance matrix
cov_matrix <- cov(bcg_data)
cat("\nCovariance Matrix:\n")
print(cov_matrix)

# c. 
#Find the Mahalanobis distances using the data and the values determined in parts (a) and (b)
library(stats)
mahalanobis_distances <- mahalanobis(bcg_data, center = column_means, cov = cov_matrix)
cat("\nMahalanobis Distances:\n")
print(mahalanobis_distances)

# d. 
#Sort the Mahalanobis distances from smallest to largest
sorted_indices <- order(mahalanobis_distances)
sorted_distances <- mahalanobis_distances[sorted_indices]
cat("\nSorted Mahalanobis Distances:\n")
print(sorted_distances)

# e. 
#Plot the sorted Mahalanobis distances vs the chi-square quantiles

df <- ncol(bcg_data)
chi_sq_quantiles <- qchisq(ppoints(length(sorted_distances)), df)
plot(chi_sq_quantiles, sorted_distances, xlab = "Chi-Square Quantiles", ylab = "Sorted Mahalanobis Distances", main = "Q-Q Plot")
abline(0, 1, col = "red")

# f. 
#Interpretation
The plot demonstrates that it is multivariate normal since the distance between the points is what is known as Mahalanobis distance.


